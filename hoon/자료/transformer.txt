Encoder : 입력된 텍스트 데이터를 숫자 혹은 벡터 형태로 변환한다. 최종적으로 가장 마지막에 출력되는 h4를 디코더 부분으로 전달하는데, h4에는 h1~h3까지의 정보가 순차적으로 포함되어 있으므로 순서를 반영할 수 있다고 본다. 
Decoder : 인코더에 의해 숫자로 변경된 정보를 다른 형태의 텍스트 데이터로 변환한다. 
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
순환신경망 방식의 문제점 

인코더에서 디코더로 넘어가는 h2 벡터는 해당 단계에서 입력된 '입니다' 단어에 대한 정보를 가장 많이 가지고 있다. 즉, 앞에서 입력된 '오늘은', '금요일'에 대한 정보는 적게 포함되어 있다.
따라서 h2를 디코더로 넘길 경우에 앞선 단어에 대한 정보가 올바르게 포함되지 않는다.  따라서 이러한 문제를 해결하기 위해서는 h0, h1 벡터의 정보를 모두 디코더로 넘길 필요가 있다. 

위 문제를 해결하기 위해서 인코더의 각 층에서 생성되는 단어에 대한 hidden state vector의 정보를 모두 디코더로 전달한다. 하지만 모든 히든 벡터를 정보를 똑같은 비중으로 전달하는 게 아니라,
예측하고자 하는 단어와의 관련이 높은 히든 벡터의 정보를 더 중요하게 전달해야 한다. 예를 들어, 'Today'를 번역하는 과정에서 '오늘은'에서 전달된 h0에 더 많은 주의(attention)를 기울인다. 
attention은 '인코더의 각 단어 벡터에 신경을 쓰는 정도'로 정의할 수 있고, 0~1 사이의 가중치로 표현한다. 

예시)

'오늘은 금요일 입니다' --> 'Today is Friday'  로 번역하는 경우

<encoder 부분> 

인코더에는 '오늘은', '금요일', '입니다' 에 대한 단어 임베딩 벡터의 정보가 입력되고, 그 결과로 각 단어의 hidden state정보인 h0, h1, h2가 출력된다. 












-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------






1. 트랜스포머 인코더 레이어:

인코더 레이어는 입력 시퀀스를 받아들이고 입력 시퀀스의 정보를 인코딩하여 중간 표현을 생성합니다.
각 인코더 레이어는 여러 개의 서브레이어로 구성됩니다. 주요 서브레이어로는 셀프 어텐션과 피드포워드 신경망이 있습니다.
셀프 어텐션은 입력 시퀀스 내의 단어들 간의 상호작용을 계산하여 각 단어의 표현을 조정합니다.
피드포워드 신경망은 셀프 어텐션 이후에 각 위치의 표현을 개선하는 데 사용됩니다.

2. 트랜스포머 디코더 레이어:

디코더 레이어는 인코딩된 정보를 바탕으로 출력 시퀀스를 생성합니다.
각 디코더 레이어는 인코더와 유사한 구조를 가지지만, 입력으로는 이전 시점의 출력과 인코딩된 정보를 받습니다.
디코더 레이어는 셀프 어텐션과 인코더-디코더 어텐션(인코딩된 정보와의 상호작용)을 사용하여 출력 시퀀스를 생성합니다.
출력 시퀀스는 다음 단어를 예측하기 위해 다음 디코더 레이어나 최종 출력 레이어로 전달됩니다.

