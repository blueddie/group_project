
이미지 캡셔닝 모델 구조
0.
데이터 전처리:

이미지를 256x256 크기로 리사이즈합니다.
캡셔닝을 단어 토큰으로 분리합니다.
단어 사전을 구축합니다. (예: 'a', 'the', 'cat', 'is', 'sitting', 'on', 'a', 'mat')
캡셔닝을 단어 토큰 인덱스 시퀀스로 변환합니다. (예: [1, 2, 3, 4, 5, 6, 1, 7])
-------------------------------------------------------------------
1. 인코더:

이미지를 입력으로 받아 이미지의 특징을 추출하는 역할
일반적으로 CNN(Convolutional Neural Network) 사용
VGGNet, ResNet, Inception 등 다양한 CNN 모델 활용 가능
이미지 특징을 벡터 형태로 변환

X: 이미지 (3채널 RGB 이미지, 256x256 크기)
y: 이미지 특징 벡터 (2048 차원)
-------------------------------------------------------------------
2. 디코더:

인코더에서 추출한 이미지 특징을 기반으로 텍스트를 생성하는 역할
일반적으로 RNN(Recurrent Neural Network) 또는 Transformer 사용
LSTM, GRU 등 다양한 RNN 모델 활용 가능
Attention 메커니즘 사용하여 이미지 특징과 텍스트 생성 과정을 연결

X: 이미지 특징 벡터 (2048 차원)
y: 캡셔닝 (단어 토큰 시퀀스)
-------------------------------------------------------------------
3. 캡셔닝 생성:

디코더에서 생성된 단어들을 조합하여 이미지를 설명하는 캡셔닝 생성
단어 순서, 문법, 의미 등을 고려하여 자연스럽고 정확한 캡셔닝 생성
Beam Search, ROUGE-L 등의 알고리즘 사용
-------------------------------------------------------------------
4. 모델 학습:

이미지와 캡셔닝 쌍 데이터를 사용하여 모델 학습
Cross-entropy loss, CIDEr loss 등을 사용하여 모델 최적화
학습률, 정규화, 드롭아웃 등의 하이퍼파라미터 조정
Adam optimizer를 사용하여 모델 학습을 수행합니다.
-------------------------------------------------------------------
